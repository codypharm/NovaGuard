# Streaming in NovaGuard ‚Äî A Tutorial

## What Is Streaming?

Without streaming:
```
User sends "hello" ‚Üí [5-15 seconds of nothing] ‚Üí Full response appears
```

With streaming:
```
User sends "hello" ‚Üí "Classifying‚Ä¶" ‚Üí "Checking FDA‚Ä¶" ‚Üí "Preparing response‚Ä¶" ‚Üí Response appears
```

Streaming lets you send **partial results** to the frontend as they happen, instead of waiting for everything to finish.

---

## The 3 Layers

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. LangGraph (astream)         ‚îÇ  ‚Üê Produces events
‚îÇ  2. FastAPI (StreamingResponse) ‚îÇ  ‚Üê Transports them as SSE
‚îÇ  3. React (ReadableStream)      ‚îÇ  ‚Üê Consumes and displays them
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Layer 1: LangGraph ‚Äî Producing Events

### Before (one-shot)
```python
# Runs the ENTIRE graph, returns only the final state
result = await workflow.ainvoke(initial_state, config)
```

### After (streaming)
```python
# Yields partial state updates as EACH NODE completes
async for chunk in workflow.astream(initial_state, config, stream_mode="updates"):
    print(chunk)
```

### What does `chunk` look like?

Each chunk is a dict where the key is the node name and the value is what that node changed:

```python
# After gateway_supervisor runs:
{"gateway_supervisor": {"intent": "CLINICAL_QUERY"}}

# After openfda runs:
{"openfda": {"drug_info_map": {"Aspirin": {...}}, "safety_flags": [...]}}

# After assistant_node runs:
{"assistant_node": {"messages": [AIMessage(content="Here is my analysis...")]}}
```

### `stream_mode` options

| Mode | What you get | Use case |
|------|-------------|----------|
| `"updates"` | Only the fields each node changed | Progress tracking (what we use) |
| `"values"` | Full state snapshot after each node | Debugging |
| `"messages"` | LLM tokens as they stream | ChatGPT-like typing effect |

We use `"updates"` because we just need to know **which node finished** and show that to the user.

---

## Layer 2: FastAPI ‚Äî SSE Transport

### What is SSE (Server-Sent Events)?

SSE is a simple protocol for sending events from server ‚Üí client over HTTP. Each event is plain text:

```
data: {"event": "progress", "node": "openfda", "label": "Checking FDA‚Ä¶"}\n\n
```

Rules:
- Each line starts with `data: `
- Each event ends with `\n\n` (double newline)
- The payload is JSON

### The FastAPI endpoint

```python
from fastapi.responses import StreamingResponse

@app.post("/clinical-interaction/stream")
async def stream_clinical_interaction(...):
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PHASE 1: Do all "setup" work here
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # This runs BEFORE the response is sent.
    # DB queries, auth checks, file reads ‚Äî all safe here.
    
    image_bytes = await file.read() if file else None
    session = await session_crud.update_session_patient(db, ...)
    workflow = request.app.state.prescription_workflow
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PHASE 2: The generator (streams to client)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # This runs AFTER the response starts.
    # ‚ö†Ô∏è The `db` session is CLOSED by now!
    
    async def event_generator():
        async for chunk in workflow.astream(initial_state, config, stream_mode="updates"):
            for node_name, node_output in chunk.items():
                if node_name.startswith("__"):
                    continue  # Skip LangGraph's internal "__end__" node
                
                label = _NODE_LABELS.get(node_name, "Processing‚Ä¶")
                
                # Format as SSE
                yield f"data: {json.dumps({'event': 'progress', 'label': label})}\n\n"
        
        # Send final result
        yield f"data: {json.dumps({'event': 'complete', 'verdict': ..., 'response': ...})}\n\n"
    
    # Return the stream
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",  # This tells the browser it's SSE
        headers={"Cache-Control": "no-cache"},
    )
```

### ‚ö†Ô∏è The #1 Gotcha: Dependency Lifecycle

This is the trap we fell into. FastAPI's `Depends()` objects have a lifecycle:

```
1. Request comes in
2. FastAPI resolves Depends() ‚Üí creates db session, authenticates user
3. Your endpoint function runs ‚Üí returns StreamingResponse
4. FastAPI CLOSES the Depends() objects (db session closed!) ‚Üê HERE
5. The generator inside StreamingResponse keeps running
6. Generator tries to use `db` ‚Üí üí• silent failure
```

**The rule:** Never use `db` or `current_user` inside the generator. Instead:

```python
# ‚úÖ CORRECT: Capture values BEFORE the generator
workflow = request.app.state.prescription_workflow  # Grab reference
_user_id = current_user.id                          # Copy the string

async def event_generator():
    # Use workflow and _user_id here ‚Äî they're just regular objects
    ...

# ‚ùå WRONG: Using Depends objects inside the generator
async def event_generator():
    await db.execute(...)        # üí• db session is closed
    user = current_user.email    # üí• might work, might not
```

If you need DB access inside the generator (like for audit logging), create a **new session**:

```python
async def event_generator():
    ...
    finally:
        from nova_guard.database import AsyncSessionLocal
        async with AsyncSessionLocal() as fresh_db:
            await save_audit_log(fresh_db, ...)
```

---

## Layer 3: React ‚Äî Consuming the Stream

### Why not EventSource?

The browser has a built-in `EventSource` API for SSE, but it **only supports GET requests**. We need POST because we send FormData (with file uploads). So we use `fetch()` + `ReadableStream`.

### The consumer function

```typescript
async function streamClinicalInteraction(
    sessionId: string, text: string, file: File | null,
    callbacks: {
        onProgress?: (event) => void,
        onComplete?: (event) => void,
        onError?: (event) => void,
    }
) {
    // 1. Send the POST request
    const res = await fetch("/clinical-interaction/stream", {
        method: "POST",
        body: formData,
    })

    // 2. Get a reader for the response body stream
    const reader = res.body!.getReader()
    const decoder = new TextDecoder()
    let buffer = ""

    // 3. Read chunks as they arrive
    while (true) {
        const { done, value } = await reader.read()
        if (done) break

        // Decode bytes ‚Üí string, append to buffer
        buffer += decoder.decode(value, { stream: true })

        // 4. Split on double newlines (SSE event boundary)
        const parts = buffer.split("\n\n")
        buffer = parts.pop() || ""  // Last part might be incomplete

        // 5. Parse each complete event
        for (const part of parts) {
            const line = part.trim()
            if (!line.startsWith("data: ")) continue

            const event = JSON.parse(line.slice(6))  // Remove "data: " prefix
            //                              ^^^^^^
            //                              "data: " is 6 characters

            switch (event.event) {
                case "progress": callbacks.onProgress?.(event); break
                case "complete": callbacks.onComplete?.(event); break
                case "error":    callbacks.onError?.(event);    break
            }
        }
    }
}
```

### Why the buffer?

Network data arrives in arbitrary chunks. One `reader.read()` might give you:

```
// Chunk 1:
"data: {\"event\":\"progress\",\"label\":\"Classif"

// Chunk 2:
"ying‚Ä¶\"}\n\ndata: {\"event\":\"progress\",\"label\":\"Checking FDA‚Ä¶\"}\n\n"
```

The buffer collects data and only processes **complete events** (terminated by `\n\n`). The incomplete tail stays in the buffer for the next iteration.

### React component wiring

```tsx
// SafetyHUD.tsx
const [processingStep, setProcessingStep] = useState<string | null>(null)

await streamClinicalInteraction(sessionId, text, file, {
    onProgress: (e) => setProcessingStep(e.label),     // Updates live
    onComplete: (e) => {
        setVerdict(e.verdict)
        setAssistantResponse(e.assistant_response)
    },
    onError: (e) => setAssistantResponse(`‚ö†Ô∏è ${e.message}`),
})

setProcessingStep(null)  // Clear when done
```

```tsx
// SafetyChat.tsx ‚Äî the processing indicator
{isProcessing && (
    <div>
        {processingStep && (
            <span className="text-teal-600 animate-pulse">
                {processingStep}  {/* "Checking FDA safety database‚Ä¶" */}
            </span>
        )}
        <BouncingDots />
    </div>
)}
```

---

## Event Types Reference

| Event | When | Payload |
|-------|------|---------|
| `progress` | Each graph node completes | `{event, node, label}` |
| `complete` | Workflow finished | `{event, status, intent, verdict, assistant_response, safety_flags}` |
| `error` | Something failed | `{event, message, detail?}` |

---

## Phase 2 (Future): Token-Level Streaming

Right now we stream **per-node** ‚Äî the user sees which step is running. Phase 2 would add **per-token** streaming for the assistant response (ChatGPT-like typing effect).

This requires:
1. Changing `stream_mode` to `"messages"` or using `astream_events()`
2. Modifying `bedrock_client.chat()` to pass `stream=True`
3. Frontend accumulating tokens character by character

Not implemented yet, but the SSE infrastructure we built supports it ‚Äî just add a new event type like `{event: "token", content: "The "}`.
